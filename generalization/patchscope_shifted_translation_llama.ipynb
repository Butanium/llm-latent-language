{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel\n",
    "from transformers import AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "import torch as th\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "import itertools\n",
    "from random import shuffle\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Fix logger bug\n",
    "import babelnet\n",
    "from nnsight import logger\n",
    "\n",
    "logger.disabled = True\n",
    "\n",
    "_ = th.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "model = \"croissantllm/CroissantLLMBase\"\n",
    "model_path = None\n",
    "check_translation_performance = False\n",
    "batch_size = 64\n",
    "thinking_langs = [\"en\", \"fr\"]\n",
    "langs = [\"en\", \"zh\", \"fr\", \"ru\", \"de\"]\n",
    "method = \"logit_lens\"\n",
    "trust_remote_code = False\n",
    "device = \"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del method  # Not used in this notebook\n",
    "langs = np.array(langs)\n",
    "out_langs = {lang: np.array([l for l in langs if l != lang]) for lang in langs}\n",
    "if model_path is None:\n",
    "    model_path = model\n",
    "tokenizer = None\n",
    "if trust_remote_code:\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "nn_model = LanguageModel(\n",
    "    model_path,\n",
    "    device_map=device,\n",
    "    torch_dtype=th.float16,\n",
    "    trust_remote_code=trust_remote_code,\n",
    "    tokenizer=tokenizer,\n",
    "    dispatch=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp_tools import (\n",
    "    run_prompts,\n",
    "    BatchPatchscopePrompt,\n",
    "    patchscope_lens_llama,\n",
    ")\n",
    "from translation_tools import translation_prompts\n",
    "\n",
    "from translation_tools import get_bn_dataset as get_translations\n",
    "\n",
    "# from translation_tools import get_gpt4_dataset as get_translations\n",
    "from utils import plot_ci, plot_k, plot_topk_tokens\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def shifted_cross_translation_plot(\n",
    "    source_input_lang,\n",
    "    source_target_lang,\n",
    "    input_lang,\n",
    "    target_lang,\n",
    "    extra_langs=None,\n",
    "    batch_size=batch_size,\n",
    "    num_words=None,\n",
    "    num_pairs=200,\n",
    "    ax=None,\n",
    "    time_=None,\n",
    "    k=4,\n",
    "):\n",
    "    \"\"\"\n",
    "    Patchscope with source hidden from:\n",
    "    index -1 and Prompt = source_input_lang: A -> source_target_lang:\n",
    "    Into target prompt:\n",
    "    into index = -1, prompt = input_lang: A -> target_lang:\n",
    "    Then plot with latent_langs, target_lang, source_target_lang\n",
    "    \"\"\"\n",
    "    if extra_langs is None:\n",
    "        extra_langs = []\n",
    "    if isinstance(extra_langs, str):\n",
    "        extra_langs = [extra_langs]\n",
    "    model_name = model.split(\"/\")[-1]\n",
    "    global source_df, target_df, target_prompts, target_probs, latent_probs, source_prompts\n",
    "    if time_ is None:\n",
    "        time_ = str(int(time()))\n",
    "    else:\n",
    "        time_ = str(time_)\n",
    "    source_df = get_translations(\n",
    "        source_input_lang,\n",
    "        [source_target_lang, input_lang, target_lang],\n",
    "        num_words,\n",
    "    )\n",
    "    target_df = get_translations(\n",
    "        input_lang,\n",
    "        [source_target_lang, source_input_lang, target_lang],\n",
    "        num_words,\n",
    "    )\n",
    "\n",
    "    _source_prompts = translation_prompts(\n",
    "        source_df,\n",
    "        nn_model.tokenizer,\n",
    "        source_input_lang,\n",
    "        source_target_lang,\n",
    "        [target_lang],\n",
    "    )\n",
    "    _target_prompts = translation_prompts(\n",
    "        target_df,\n",
    "        nn_model.tokenizer,\n",
    "        input_lang,\n",
    "        target_lang,\n",
    "        [source_target_lang, *extra_langs],\n",
    "    )\n",
    "\n",
    "    collected_pairs = 0\n",
    "    source_prompts = []\n",
    "    target_prompts = []\n",
    "    source_target = list(itertools.product(source_df.iterrows(), target_df.iterrows()))\n",
    "    shuffle(source_target)\n",
    "\n",
    "    for (i, source_row), (j, target_row) in source_target:\n",
    "        if source_row[\"word_original\"] == target_row[\"word_original\"]:\n",
    "            continue\n",
    "        # Check for token overlap\n",
    "        source_prompts.append(deepcopy(_source_prompts[i]))\n",
    "        target_prompts.append(deepcopy(_target_prompts[j]))\n",
    "        collected_pairs += 1\n",
    "        if collected_pairs >= num_pairs:\n",
    "            break\n",
    "\n",
    "    for targ_p, src_p in zip(target_prompts, source_prompts):\n",
    "        targ_p.latent_tokens[f\"source_{source_target_lang}\"] = src_p.target_tokens\n",
    "        targ_p.latent_tokens[f\"source_{target_lang}\"] = src_p.latent_tokens[target_lang]\n",
    "        targ_p.latent_strings[f\"source_{source_target_lang}\"] = src_p.target_string\n",
    "        targ_p.latent_strings[f\"source_{target_lang}\"] = src_p.latent_strings[\n",
    "            target_lang\n",
    "        ]\n",
    "    source_prompts_str = [p.prompt for p in source_prompts]\n",
    "\n",
    "    def transverse_patchscope(nn_model, prompt_batch, scan):\n",
    "        offset = transverse_patchscope.offset\n",
    "        target_pathscope_prompts = BatchPatchscopePrompt.from_prompts(prompt_batch, -1)\n",
    "        source_prompt_batch = source_prompts_str[offset : offset + len(prompt_batch)]\n",
    "        transverse_patchscope.offset += len(prompt_batch)\n",
    "        return patchscope_lens_llama(\n",
    "            nn_model, source_prompt_batch, target_pathscope_prompts, scan=scan\n",
    "        )\n",
    "\n",
    "    transverse_patchscope.offset = 0\n",
    "    target_probs, latent_probs = run_prompts(\n",
    "        nn_model, target_prompts, batch_size=batch_size, method=transverse_patchscope\n",
    "    )\n",
    "\n",
    "    # Get the baseline to normalize the plots\n",
    "    source_prompts_probs, _ = run_prompts(\n",
    "        nn_model, source_prompts, batch_size=batch_size, method=\"next_token_probs\"\n",
    "    )\n",
    "    source_prompts_baseline = source_prompts_probs.mean()\n",
    "    target_prompts_probs, _ = run_prompts(\n",
    "        nn_model, target_prompts, batch_size=batch_size, method=\"next_token_probs\"\n",
    "    )\n",
    "    target_prompts_baseline = target_prompts_probs.mean()\n",
    "\n",
    "    json_dic = {\n",
    "        target_lang: target_probs.tolist(),\n",
    "        \"source prompt probs\": source_prompts_probs.squeeze().tolist(),\n",
    "        \"target prompt probs\": target_prompts_probs.squeeze().tolist(),\n",
    "    }\n",
    "    for label, probs in latent_probs.items():\n",
    "        json_dic[label] = probs.tolist()\n",
    "    path = (\n",
    "        Path(\"results\")\n",
    "        / model_name\n",
    "        / \"shifted_cross_translation\"\n",
    "        / (f\"{source_input_lang}_{source_target_lang}-{input_lang}_{target_lang}-\")\n",
    "    )\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    json_file = path / (time_ + \".json\")\n",
    "    with open(json_file, \"w\") as f:\n",
    "        json.dump(json_dic, f, indent=4)\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    colors = sns.color_palette(\"tab10\", 4 + len(extra_langs))\n",
    "    plot_ci(\n",
    "        ax, target_probs / target_prompts_baseline, label=target_lang, color=colors[0]\n",
    "    )\n",
    "    for i, (label, probs) in enumerate(latent_probs.items()):\n",
    "        if \"source\" in label:\n",
    "            baseline = source_prompts_baseline\n",
    "        else:\n",
    "            baseline = target_prompts_baseline\n",
    "        plot_ci(ax, probs / baseline, label=label, color=colors[i + 1], init=False)\n",
    "    ax.legend()\n",
    "    title = f\"{model_name}: HeteroPatch from ({source_input_lang} -> {source_target_lang}) into ({input_lang} -> {target_lang})\"\n",
    "    ax.set_title(title)\n",
    "    # Save the plot\n",
    "    plot_file = path / (time_ + \".png\")\n",
    "    plt.savefig(plot_file, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    # change the size\n",
    "    fig, axes = plt.subplots(1, k, figsize=(5 * k, 5))\n",
    "\n",
    "    plot_k(\n",
    "        axes,\n",
    "        target_probs[:k],\n",
    "        label=target_lang,\n",
    "        color=colors[0],\n",
    "        k=k,\n",
    "    )\n",
    "    for i, (label, probs) in enumerate(latent_probs.items()):\n",
    "        plot_k(\n",
    "            axes,\n",
    "            probs[:k],\n",
    "            label=label,\n",
    "            color=colors[i + 1],\n",
    "            init=False,\n",
    "            k=k,\n",
    "        )\n",
    "    axes[-1].legend()\n",
    "    fig.suptitle(title + \" - Raw probabilities\")\n",
    "    plt_file = path / (time_ + \"_k.png\")\n",
    "    fig.savefig(plt_file, dpi=300, bbox_inches=\"tight\")\n",
    "    fig.show()\n",
    "    # Compute a single example\n",
    "    json_meta = {}\n",
    "    for i in range(k):\n",
    "        json_meta[i] = {\n",
    "            \"source input lang\": source_input_lang,\n",
    "            \"source target lang\": source_target_lang,\n",
    "            \"input lang\": input_lang,\n",
    "            \"target lang\": target_lang,\n",
    "            \"source prompt\": source_prompts_str[i],\n",
    "            \"source prompt target\": source_prompts[i].target_string,\n",
    "            \"source prompt latent\": source_prompts[i].latent_strings,\n",
    "            \"target prompt\": target_prompts[i].prompt,\n",
    "            \"target prompt target\": target_prompts[i].target_string,\n",
    "            \"target prompt latent\": target_prompts[i].latent_strings,\n",
    "        }\n",
    "    json_df = pd.DataFrame(json_meta)\n",
    "    with pd.option_context(\n",
    "        \"display.max_colwidth\",\n",
    "        None,\n",
    "        \"display.max_columns\",\n",
    "        None,\n",
    "        \"display.max_rows\",\n",
    "        None,\n",
    "    ):\n",
    "        display(json_df)\n",
    "    target_prompt_batch = BatchPatchscopePrompt.from_prompts(\n",
    "        [p.prompt for p in target_prompts[:k]], -1\n",
    "    )\n",
    "    probs = patchscope_lens_llama(nn_model, source_prompts_str[:k], target_prompt_batch)\n",
    "    file = path / (time_ + \"_heatmap.png\")\n",
    "    plot_topk_tokens(probs, nn_model, title=title, file=file)\n",
    "\n",
    "    meta_file = path / (time_ + \"_heatmap.meta.json\")\n",
    "    with open(meta_file, \"w\") as f:\n",
    "        json.dump(json_meta, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_ = int(time())\n",
    "print(f\"Experiment time id: {time_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source_input_lang in langs:\n",
    "    for source_target_lang in out_langs[source_input_lang]:\n",
    "        for in_lang in langs:\n",
    "            for out_lang in out_langs[in_lang]:\n",
    "                th.cuda.empty_cache()\n",
    "                shifted_cross_translation_plot(\n",
    "                    source_input_lang,\n",
    "                    source_target_lang,\n",
    "                    in_lang,\n",
    "                    out_lang,\n",
    "                    time_=time_,\n",
    "                )\n",
    "                plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
