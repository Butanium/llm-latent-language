{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "import itertools\n",
    "from random import shuffle\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Fix logger bug\n",
    "import babelnet\n",
    "from nnsight import logger\n",
    "\n",
    "logger.disabled = True\n",
    "\n",
    "_ = th.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"{YOUR EXP NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Papermill args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "langs = [\"fr\", \"de\", \"ru\", \"en\", \"zh\"]\n",
    "batch_size = 8\n",
    "model = \"Llama-2-7b\"\n",
    "device = \"auto\"\n",
    "# model_path = \"/dlabscratch1/public/llm_weights/llama2_hf/Llama-2-7b-hf\"\n",
    "model_path = None\n",
    "trust_remote_code = False\n",
    "extra_args = []\n",
    "exp_id = None\n",
    "paper_only = False\n",
    "skip_paper = False\n",
    "prob_treshold = 0.3\n",
    "map_source_lang = None\n",
    "map_source_lang_kwargs = {}\n",
    "map_target_lang = None\n",
    "map_target_lang_kwargs = {}\n",
    "use_tl = False\n",
    "num_few_shot = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CL Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "\n",
    "parser = ArgumentParser()\n",
    "args = parser.parse_args(extra_args)\n",
    "print(f\"args: {args}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and arg preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp_tools import load_model\n",
    "import prompt_tools\n",
    "from functools import partial\n",
    "\n",
    "model_name = model.split(\"/\")[-1]\n",
    "langs = np.array(langs)\n",
    "if model_path is None:\n",
    "    model_path = model\n",
    "nn_model = load_model(\n",
    "    model_path,\n",
    "    trust_remote_code=trust_remote_code,\n",
    "    device_map=device,\n",
    "    use_tl=use_tl,\n",
    "    # dispatch=True,\n",
    ")\n",
    "tokenizer = nn_model.tokenizer\n",
    "\n",
    "if isinstance(map_source_lang, str):\n",
    "    map_source_lang = getattr(prompt_tools, map_source_lang)\n",
    "    map_source_lang = partial(map_source_lang, **map_source_lang_kwargs)\n",
    "if isinstance(map_target_lang, str):\n",
    "    map_target_lang = getattr(prompt_tools, map_target_lang)\n",
    "    map_target_lang = partial(map_target_lang, **map_target_lang_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp_tools import (\n",
    "    run_prompts,\n",
    "    next_token_probs,\n",
    "    filter_prompts_by_prob,\n",
    "    remove_colliding_prompts,\n",
    ")\n",
    "from prompt_tools import translation_prompts\n",
    "from translation_tools import get_bn_dataset as get_translations\n",
    "\n",
    "from display_utils import plot_topk_tokens, k_subplots, plot_results, plot_k_results\n",
    "\n",
    "\n",
    "def your_ploting_func(\n",
    "    input_lang,\n",
    "    target_lang,\n",
    "    extra_langs=None,\n",
    "    batch_size=batch_size,\n",
    "    num_words=None,\n",
    "    exp_id=None,\n",
    "    num_examples=9,\n",
    "):\n",
    "    \"\"\"\n",
    "    func docstring\n",
    "    \"\"\"\n",
    "    if extra_langs is None:\n",
    "        extra_langs = []\n",
    "    if isinstance(extra_langs, str):\n",
    "        extra_langs = [extra_langs]\n",
    "    # global foo  # Var you might want to access for debugging purposes\n",
    "    # foo = 2\n",
    "    df = get_translations(\n",
    "        input_lang,\n",
    "        [target_lang, *extra_langs],\n",
    "        num_words=num_words,\n",
    "    )\n",
    "    no_proc = []\n",
    "    if map_source_lang is not None:\n",
    "        df[input_lang + \" no proc\"] = df[input_lang]\n",
    "        df[input_lang] = df[input_lang].apply(map_source_lang)\n",
    "        no_proc.append(input_lang + \" no proc\")\n",
    "    if map_target_lang is not None:\n",
    "        df[target_lang + \" no proc\"] = df[target_lang]\n",
    "        df[target_lang] = df[target_lang].apply(map_target_lang)\n",
    "        no_proc.append(target_lang + \" no proc\")\n",
    "\n",
    "    target_prompts = translation_prompts(\n",
    "        df, tokenizer, input_lang, target_lang, extra_langs, n=num_few_shot\n",
    "    )\n",
    "    target_prompts = remove_colliding_prompts(target_prompts, ignore_langs=no_proc)\n",
    "    print(f\"Number of non-colliding prompts: {len(target_prompts)}\")\n",
    "    target_prompts = filter_prompts_by_prob(\n",
    "        target_prompts, nn_model, prob_treshold, batch_size=batch_size\n",
    "    )\n",
    "    print(f\"Number of prompts after filtering: {len(target_prompts)}\")\n",
    "    if len(target_prompts) < num_examples:\n",
    "        print(\"Not enough prompts after filtering\")\n",
    "        return\n",
    "\n",
    "    def get_prob_func(nn_model, prompt_batch, scan):\n",
    "        return next_token_probs(\n",
    "            nn_model,\n",
    "            prompt_batch,\n",
    "        ).unsqueeze(1)\n",
    "\n",
    "    target_probs, latent_probs = run_prompts(\n",
    "        nn_model, target_prompts, batch_size=batch_size, get_probs=get_prob_func\n",
    "    )\n",
    "\n",
    "    json_dic = {\n",
    "        target_lang: target_probs.tolist(),\n",
    "    }\n",
    "    for label, probs in latent_probs.items():\n",
    "        json_dic[label] = probs.tolist()\n",
    "    path = (\n",
    "        Path(\"results\") / model_name / exp_name / (f\"{input_lang}_{target_lang}\")\n",
    "    )\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    json_file = path / (exp_id + \".json\")\n",
    "    with open(json_file, \"w\") as f:\n",
    "        json.dump(json_dic, f, indent=4)\n",
    "    print(f\"Saved data to {json_file.resolve()}\")\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    pref = pref.replace(\"_\", \" \")\n",
    "    title = (\n",
    "        f\"{model_name}: {exp_name} from ({pref}) into ({input_lang} -> {target_lang})\"\n",
    "    )\n",
    "    plot_results(ax, target_probs, latent_probs, target_lang)\n",
    "    ax.legend()\n",
    "    ax.set_title(title)\n",
    "    plt.tight_layout()\n",
    "    plot_file = path / (exp_id + \".png\")\n",
    "    plt.savefig(plot_file, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot k examples\n",
    "    fig, axes = k_subplots(num_examples)\n",
    "    plot_k_results(axes, target_probs, latent_probs, target_lang, num_examples)\n",
    "    axes[num_examples - 1].legend()\n",
    "    fig.suptitle(title)\n",
    "    plt_file = path / (exp_id + \"_k.png\")\n",
    "    fig.savefig(plt_file, dpi=300, bbox_inches=\"tight\")\n",
    "    fig.show()\n",
    "    # Compute a single example\n",
    "    json_heatmap = {}\n",
    "    for i in range(num_examples):\n",
    "        json_heatmap[i] = {\n",
    "            \"input lang\": input_lang,\n",
    "            \"target lang\": target_lang,\n",
    "            \"target prompt\": target_prompts[i].prompt,\n",
    "            \"target prompt target\": target_prompts[i].target_strings,\n",
    "            \"target prompt latent\": target_prompts[i].latent_strings,\n",
    "        }\n",
    "    json_df = pd.DataFrame(json_heatmap)\n",
    "    with pd.option_context(\n",
    "        \"display.max_colwidth\",\n",
    "        None,\n",
    "        \"display.max_columns\",\n",
    "        None,\n",
    "        \"display.max_rows\",\n",
    "        None,\n",
    "    ):\n",
    "        display(json_df)\n",
    "    target_prompt_batch = [p.prompt for p in target_prompts[:num_examples]]\n",
    "    probs = get_prob_func(\n",
    "        nn_model,\n",
    "        target_prompt_batch,\n",
    "        scan=True,\n",
    "    )\n",
    "    file = path / (exp_id + \"_heatmap.png\")\n",
    "    plot_topk_tokens(probs, nn_model, title=title, file=file)\n",
    "\n",
    "    meta_file = path / (exp_id + \"_heatmap.meta.json\")\n",
    "    with open(meta_file, \"w\") as f:\n",
    "        json.dump(json_heatmap, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selected args for the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_paper:\n",
    "    paper_args = []\n",
    "    for f_args in paper_args:\n",
    "        th.cuda.empty_cache()\n",
    "        your_ploting_func(*f_args, exp_id=exp_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not paper_only:\n",
    "    for in_lang in langs:\n",
    "        for out_lang in langs:\n",
    "            if in_lang == out_lang:\n",
    "                continue\n",
    "            # ... more nested loops\n",
    "            th.cuda.empty_cache()\n",
    "            your_ploting_func(\n",
    "                in_lang,\n",
    "                out_lang,\n",
    "                exp_id=exp_id,\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
