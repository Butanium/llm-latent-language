{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch as th\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "import itertools\n",
    "from random import shuffle\n",
    "\n",
    "# Fix logger bug\n",
    "import babelnet\n",
    "from nnsight import logger\n",
    "\n",
    "logger.disabled = True\n",
    "\n",
    "_ = th.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "model = \"croissantllm/CroissantLLMBase\"\n",
    "model_path = None\n",
    "check_translation_performance = False\n",
    "batch_size = 64\n",
    "thinking_langs = [\"en\", \"fr\"]\n",
    "langs = [\"en\", \"zh\", \"fr\", \"ru\", \"de\"]\n",
    "method = \"logit_lens\"\n",
    "trust_remote_code = False\n",
    "device = \"auto\"\n",
    "exp_id = \"test\"\n",
    "paper_only = False\n",
    "num_few_shot = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp_tools import load_model\n",
    "del method  # Not used in this notebook\n",
    "langs = np.array(langs)\n",
    "out_langs = {lang: np.array([l for l in langs if l != lang]) for lang in langs}\n",
    "if model_path is None:\n",
    "    model_path = model\n",
    "nn_model = load_model(\n",
    "    model_path,\n",
    "    trust_remote_code=trust_remote_code,\n",
    "    device_map=device,\n",
    "    dispatch=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp_tools import run_prompts, patchscope_lens, next_token_probs_unsqueeze\n",
    "from interventions import TargetPromptBatch\n",
    "from prompt_tools import translation_prompts, get_shifted_prompt_pairs\n",
    "\n",
    "from translation_tools import get_bn_dataset as get_translations\n",
    "\n",
    "from utils import ulist\n",
    "from display_utils import plot_k_results, plot_topk_tokens, plot_results, k_subplots\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def shifted_cross_translation_plot(\n",
    "    source_input_lang,\n",
    "    source_output_lang,\n",
    "    input_lang,\n",
    "    target_lang,\n",
    "    extra_langs=None,\n",
    "    batch_size=batch_size,\n",
    "    num_words=None,\n",
    "    num_pairs=200,\n",
    "    exp_id=None,\n",
    "    k=4,\n",
    "):\n",
    "    \"\"\"\n",
    "    Patchscope with source hidden from:\n",
    "    index -1 and Prompt = source_input_lang: A -> source_target_lang:\n",
    "    Into target prompt:\n",
    "    into index = -1, prompt = input_lang: A -> target_lang:\n",
    "    Then plot with latent_langs, target_lang, source_target_lang\n",
    "    \"\"\"\n",
    "    if extra_langs is None:\n",
    "        extra_langs = []\n",
    "    if isinstance(extra_langs, str):\n",
    "        extra_langs = [extra_langs]\n",
    "    model_name = model.split(\"/\")[-1]\n",
    "    global source_df, target_df, target_prompts, target_probs, latent_probs, source_prompts\n",
    "    if exp_id is None:\n",
    "        exp_id = str(int(time()))\n",
    "    else:\n",
    "        exp_id = str(exp_id)\n",
    "    source_df = get_translations(\n",
    "        source_input_lang,\n",
    "        ulist([source_output_lang, input_lang, target_lang, *extra_langs]),\n",
    "        num_words,\n",
    "    )\n",
    "    target_df = get_translations(\n",
    "        input_lang,\n",
    "        ulist([source_output_lang, source_input_lang, target_lang, *extra_langs]),\n",
    "        num_words,\n",
    "    )\n",
    "\n",
    "    _source_prompts = translation_prompts(\n",
    "        source_df,\n",
    "        nn_model.tokenizer,\n",
    "        source_input_lang,\n",
    "        source_output_lang,\n",
    "        [target_lang, *extra_langs],\n",
    "        augment_tokens=False,\n",
    "        n=num_few_shot,\n",
    "    )\n",
    "    _target_prompts = translation_prompts(\n",
    "        target_df,\n",
    "        nn_model.tokenizer,\n",
    "        input_lang,\n",
    "        target_lang,\n",
    "        [source_output_lang, *extra_langs],\n",
    "        augment_tokens=False,\n",
    "        n=num_few_shot,\n",
    "    )\n",
    "\n",
    "    source_prompts, target_prompts = get_shifted_prompt_pairs(\n",
    "        source_df,\n",
    "        target_df,\n",
    "        _source_prompts,\n",
    "        _target_prompts,\n",
    "        source_input_lang,\n",
    "        source_output_lang,\n",
    "        input_lang,\n",
    "        target_lang,\n",
    "        extra_langs,\n",
    "        num_pairs,\n",
    "        merge_extra_langs=True,\n",
    "    )\n",
    "\n",
    "    source_prompts_str = [p.prompt for p in source_prompts]\n",
    "\n",
    "    def transverse_patchscope(nn_model, prompt_batch, scan):\n",
    "        offset = transverse_patchscope.offset\n",
    "        target_pathscope_prompts = TargetPromptBatch.from_prompts(prompt_batch, -1)\n",
    "        source_prompt_batch = source_prompts_str[offset : offset + len(prompt_batch)]\n",
    "        transverse_patchscope.offset += len(prompt_batch)\n",
    "        return patchscope_lens(\n",
    "            nn_model, source_prompt_batch, target_pathscope_prompts, scan=scan\n",
    "        )\n",
    "\n",
    "    transverse_patchscope.offset = 0\n",
    "    target_probs, latent_probs = run_prompts(\n",
    "        nn_model, target_prompts, batch_size=batch_size, get_probs=transverse_patchscope\n",
    "    )\n",
    "\n",
    "    # Get the baseline to normalize the plots\n",
    "    source_prompts_probs, _ = run_prompts(\n",
    "        nn_model,\n",
    "        source_prompts,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    target_prompts_probs, _ = run_prompts(\n",
    "        nn_model,\n",
    "        target_prompts,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    json_dic = {\n",
    "        target_lang: target_probs.tolist(),\n",
    "        \"source prompt probs\": source_prompts_probs.squeeze().tolist(),\n",
    "        \"target prompt probs\": target_prompts_probs.squeeze().tolist(),\n",
    "    }\n",
    "    for label, probs in latent_probs.items():\n",
    "        json_dic[label] = probs.tolist()\n",
    "    path = (\n",
    "        Path(\"results\")\n",
    "        / model_name\n",
    "        / \"shifted_cross_translation\"\n",
    "        / (f\"{source_input_lang}_{source_output_lang}-{input_lang}_{target_lang}-\")\n",
    "    )\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    json_file = path / (exp_id + \".json\")\n",
    "    with open(json_file, \"w\") as f:\n",
    "        json.dump(json_dic, f, indent=4)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "    # Raw probabilities plot\n",
    "    plot_results(ax, target_probs, latent_probs, target_lang)\n",
    "    ax.legend()\n",
    "    title = f\"{model_name}: HeteroPatch from ({source_input_lang} -> {source_output_lang}) into ({input_lang} -> {target_lang})\"\n",
    "    ax.set_title(title)\n",
    "    plt.tight_layout()\n",
    "    plot_file = path / (exp_id + \".png\")\n",
    "    plt.savefig(plot_file, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot k examples\n",
    "    fig, axes = k_subplots(k)\n",
    "    plot_k_results(axes, target_probs, latent_probs, target_lang, k=k)\n",
    "    axes[k - 1].legend()\n",
    "    fig.suptitle(title)\n",
    "    plt_file = path / (exp_id + \"_k.png\")\n",
    "    fig.savefig(plt_file, dpi=300, bbox_inches=\"tight\")\n",
    "    fig.show()\n",
    "    # Compute a single example\n",
    "    json_meta = {}\n",
    "    for i in range(k):\n",
    "        json_meta[i] = {\n",
    "            \"source input lang\": source_input_lang,\n",
    "            \"source target lang\": source_output_lang,\n",
    "            \"input lang\": input_lang,\n",
    "            \"target lang\": target_lang,\n",
    "            \"source prompt\": source_prompts_str[i],\n",
    "            \"source prompt target\": source_prompts[i].target_strings,\n",
    "            \"source prompt latent\": source_prompts[i].latent_strings,\n",
    "            \"target prompt\": target_prompts[i].prompt,\n",
    "            \"target prompt target\": target_prompts[i].target_strings,\n",
    "            \"target prompt latent\": target_prompts[i].latent_strings,\n",
    "        }\n",
    "    json_df = pd.DataFrame(json_meta)\n",
    "    with pd.option_context(\n",
    "        \"display.max_colwidth\",\n",
    "        None,\n",
    "        \"display.max_columns\",\n",
    "        None,\n",
    "        \"display.max_rows\",\n",
    "        None,\n",
    "    ):\n",
    "        display(json_df)\n",
    "    target_prompt_batch = TargetPromptBatch.from_prompts(\n",
    "        [p.prompt for p in target_prompts[:k]], -1\n",
    "    )\n",
    "    probs = patchscope_lens(nn_model, source_prompts_str[:k], target_prompt_batch)\n",
    "    file = path / (exp_id + \"_heatmap.png\")\n",
    "    plot_topk_tokens(probs, nn_model, title=title, file=file)\n",
    "\n",
    "    meta_file = path / (exp_id + \"_heatmap.meta.json\")\n",
    "    with open(meta_file, \"w\") as f:\n",
    "        json.dump(json_meta, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_args = [\n",
    "    (\"de\", \"it\", \"fr\", \"zh\"),\n",
    "    (\"it\", \"de\", \"fr\", \"zh\"),\n",
    "    # (\"de\", \"fr\", \"zh\", \"ru\"),\n",
    "    # (\"de\", \"fr\", \"it\", \"zh\"),\n",
    "    # (\"de\", \"en\", \"fr\", \"zh\"),\n",
    "    # ('zh', 'fr', 'de', 'zh'),\n",
    "    # ('zh', 'de', 'fr', 'zh'),\n",
    "    # (\"fr\", \"zh\", \"en\", \"de\"),\n",
    "    # ('zh', 'en', 'fr', 'zh'),\n",
    "    # (\"fr\", \"zh\", \"ru\", \"en\"),\n",
    "    # (\"fr\", \"zh\", \"zh\", \"en\"),\n",
    "    # (\"de\", \"en\", \"de\", \"zh\"),\n",
    "    # (\"de\", \"en\", \"fr\", \"zh\")\n",
    "]\n",
    "for args in paper_args:\n",
    "    shifted_cross_translation_plot(*args, exp_id=exp_id, extra_langs=[\"en\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not paper_only:\n",
    "    for source_input_lang in langs:\n",
    "        for source_output_lang in out_langs[source_input_lang]:\n",
    "            for in_lang in langs:\n",
    "                for out_lang in out_langs[in_lang]:\n",
    "                    th.cuda.empty_cache()\n",
    "                    shifted_cross_translation_plot(\n",
    "                        source_input_lang,\n",
    "                        source_output_lang,\n",
    "                        in_lang,\n",
    "                        out_lang,\n",
    "                        exp_id=exp_id,\n",
    "                    )\n",
    "                    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
